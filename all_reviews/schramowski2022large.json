{"schramowski2022large": {"title": "Large pre-trained language models contain human-like biases of what is right and wrong to do", "author": "Schramowski, Patrick and Turan, Cigdem and Andersen, Nico and Rothkopf, Constantin A and Kersting, Kristian", "year": "2022", "bibKey": "schramowski2022large", "bibtexFile": {}, "surveyFile": {}, "typeApplicationAI": "typeApplicationAI", "typeNotRelevantText": "--", "paperContentLength": "18", "paperTotalLength": "38", "includesLLMPrompt": "includesLLMPrompt", "includesProbing": "includesProbing", "paperMotivationText": "Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, its variants, GPT-2/3, and others. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour.", "paperContributionText": "The authors show that recent LMs contain human-like biases of what is right and wrong to do, some form of ethical and moral norms of the society \u2014they bring a \u201cmoral direction\u201d to surface. They show that these norms can be captured geometrically by a direction, which can be computed in the embedding space, reflecting well the agreement of phrases to social norms implicitly expressed in the training texts and providing a path for attenuating or even preventing toxic degeneration in LMs.", "paperResultsText": "Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, the authors demonstrate the capabilities of the \u201cmoral direction\u201d for guiding (even other) LMs towards producing normative text and showcase it on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2.", "theoryOtherText": "--", "theoryOwnText": "--", "theoryNone": "theoryNone", "definition": "definitionYes", "unitSentence": "unitSentence", "unitToken": "unitToken", "goalAI": "goalAI", "langEn": "langEn", "langOther": "--", "dataOther": "dataOther", "dataOtherText": "Mixed moral statements", "dataDomain": "dataDomainOther", "dataDomainOtherText": "Mixed moral statements", "resourcesEthics": "resourcesEthics", "resourcesOther": "resourcesOther", "resourcesOtherText": "RealToxicityPrompts (Gehman et al., 2020)", "annotSizeText": "--", "annotIAATypeText": "--", "annotIAAScoreText": "--", "annotIAAMetricText": "--", "AnnotResourceAvailableYesURL": "--", "AnnotResourceAvailablePartlyURL": "--", "resource": "resourceNo", "experiment": "experimentYes", "expTransformerText": "--", "expLlm": "expLlm", "expLlmText": "BERT, LAMA, GPT-2", "semiMLText": "--", "unsuperMLText": "--", "expOtherText": "--", "ExpErrAnalysis": "ExpErrAnalysisNo", "replicTrainTest": "replicTrainTestNotRelevant", "replicGold": "replicGoldClear", "analysisFieldOtherText": "--", "analysis": "analysisNo", "dataAvail": "dataAvailYes", "dataYesUrlText": "https://github.com/ml-research/MoRT_NMI/tree/master/ Supplemental_Material/UserStudy, https: //hessenbox.tu-darmstadt.de/public?folderID=MjR2QVhvQmc0blFpdWd1YjViNHpz", "dataYesCommentText": "--", "dataPartlyUrlText": "--", "replicPreproc": "replicPreprocNotRelevant", "replicCode": "replicCodeYes", "replicCodeText": "https: //github.com/ml-research/MoRT_NMI", "validationAnnotation": "validationAnnotation", "validationCorrelation": "validationCorrelation", "validationOtherText": "--"}}