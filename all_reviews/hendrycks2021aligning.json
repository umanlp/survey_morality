{"hendrycks2021aligning": {"title": "Aligning AI With Shared Human Values", "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew Critch and Li, Jerry Li and Song, Dawn and Steinhardt, Jacob", "year": "2021", "bibKey": "hendrycks2021aligning", "bibtexFile": {}, "surveyFile": {}, "typeResource": "typeResource", "typeExperiment": "typeExperiment", "typeApplicationAI": "typeApplicationAI", "typeNotRelevantText": "--", "paperContentLength": "9", "paperTotalLength": "29", "includesAnnotation": "includesAnnotation", "includesSupervised": "includesSupervised", "includesLLMPrompt": "includesLLMPrompt", "includesProbing": "includesProbing", "paperMotivationText": "Embedding ethics into AI systems remains an outstanding challenge without any concrete proposal. The demand for ethical machine learning (White House, 2016; European Commission, 2019) has already led researchers to propose various ethical principles for narrow applications. To make algorithms more fair, researchers have proposed precise mathematical criteria. However, many of these fairness criteria have been shown to be mutually incompatible (Kleinberg et al., 2017), and these rigid formalizations are task-specific and have been criticized for being simplistic.", "paperContributionText": "We show how to assess a language model\u2019s knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality.", "paperResultsText": "We find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.", "theoryOtherText": "--", "theoryOwnText": "--", "theoryNone": "theoryNone", "definition": "definitionVague", "unitDocument": "unitDocument", "goalPerson": "goalPerson", "goalAI": "goalAI", "langEn": "langEn", "langOther": "--", "dataSM": "dataSM", "dataSMReddit": "dataSMReddit", "dataOther": "dataOther", "dataOtherText": "Elicited moral situations", "dataDomain": "dataDomainOther", "dataDomainOtherText": "Mixed moral situations", "resourcesEthics": "resourcesEthics", "resourcesOtherText": "--", "resource": "resourceYes", "annotSizeText": "130,000 examples (elicited moral scenarious and comments from Reddit)", "annotSetup": "annotCrowd", "annotViews": "annotViewsNo", "AnnotSchema": "AnnotSchemaYes", "AnnotSchemaLen": "AnnotSchemaLen2", "IAA": "IAAYes", "annotIAATypeText": "Morality score", "annotIAAScoreText": "93,9", "annotIAAMetricText": "Solely agreement in percentage, no metric used ", "AnnotErrAnalysis": "AnnotErrAnalysisRudimentary", "AnnotResourceAvailable": "AnnotResourceAvailableYes", "AnnotResourceAvailableYesURL": "https://github.com/hendrycks/ethics", "AnnotResourceAvailablePartlyURL": "--", "experiment": "experimentYes", "expTransformers": "expTransformers", "expTransformerText": "BERT, RoBERTa, ALBERT", "expLlm": "expLlm", "expLlmText": "GPT-3", "semiMLText": "--", "unsuperMLText": "--", "expOtherText": "--", "ExpErrAnalysis": "ExpErrAnalysisYes", "replicTrainTest": "replicTrainTestYes", "replicGold": "replicGoldClear", "analysisFieldOtherText": "--", "analysis": "analysisNo", "dataAvail": "dataAvailYes", "dataYesUrlText": "https://github.com/hendrycks/ethics", "dataYesCommentText": "--", "dataPartlyUrlText": "--", "replicPreproc": "replicPreprocClear", "replicCode": "replicCodeYes", "replicCodeText": "https://github.com/hendrycks/ethics", "validationAnnotation": "validationAnnotation", "validationOtherText": "--"}}