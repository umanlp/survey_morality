{"DBLP:conf/aies/JentzschSRK19": {"title": "Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices", "author": "Sophie F. Jentzsch and\nPatrick Schramowski and\nConstantin A. Rothkopf and\nKristian Kersting", "year": "2019", "bibKey": "DBLP:conf/aies/JentzschSRK19", "bibtexFile": {}, "surveyFile": {}, "typeApplicationAI": "typeApplicationAI", "typeNotRelevantText": "--", "paperContentLength": "6", "paperTotalLength": "8", "includesUnsupervised": "includesUnsupervised", "includesProbing": "includesProbing", "paperMotivationText": "The authors investigate whether models reflect human-like choices when faced with morally relevant questions (e.g. \"Should I put my hamster into the toaster?\").", "paperContributionText": "The authors develop a framework named \"Moral Choice Machine\" that chooses between the answers 'yes' or 'no' when faced with a morally relevant question. The method is based on cosine similarity between sentence embeddings for the positive and negative answers, given the input question. They analyze moral biases in this moral choice machine using template-based question-answer pairs.", "paperResultsText": "Correlation with WEAT scores shows that the moral choice machine reflects human-like moral choices. The authors also reveal gender biases in the model.", "theoryOtherText": "--", "theoryOwnText": "--", "theoryNone": "theoryNone", "definition": "definitionVague", "unitSegment": "unitSegment", "goalAI": "goalAI", "langEn": "langEn", "langOther": "--", "dataOther": "dataOther", "dataOtherText": "artificial question-answer pairs", "dataDomain": "dataDomainOther", "dataDomainOtherText": "artificial data (morally relevant question-answer pairs)", "resourcesOther": "resourcesOther", "resourcesOtherText": "AFINN lexicon", "annotSizeText": "--", "annotIAATypeText": "--", "annotIAAScoreText": "--", "annotIAAMetricText": "--", "AnnotResourceAvailableYesURL": "--", "AnnotResourceAvailablePartlyURL": "--", "resource": "resourceNo", "experiment": "experimentYes", "expTransformerText": "--", "expLlmText": "--", "semiMLText": "--", "unsuperML": "unsuperML", "unsuperMLText": "cosine similarity of available sentence embeddings that were pre-trained on news/Wikipedia by Google (no fine-tuning performed by the authors of the paper)", "expOtherText": "--", "ExpErrAnalysis": "ExpErrAnalysisNotRelevant", "replicTrainTest": "replicTrainTestNotRelevant", "replicGold": "replicGoldNotRelevant", "analysisFieldOtherText": "--", "analysis": "analysisNo", "dataYesUrlText": "--", "dataYesCommentText": "--", "dataPartlyUrlText": "--", "dataAvail": "dataAvailNotRelevant", "replicPreproc": "replicPreprocClear", "replicCode": "replicCodeYes", "replicCodeText": "https://github.com/ml-research/moral-choice-machine", "validationCorrelation": "validationCorrelation", "validationOtherText": "--"}}